{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271d6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1045e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Environment Vars\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "#os.environ[\"LANGCHAIN_PROJECT\"] = \"ConversationalAIBot\"\n",
    "#os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9322e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatGroq(\n",
    "    temperature = 0.2,\n",
    "    model_name = \"deepseek-r1-distill-llama-70b\",\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a Research assistant from arxiv\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question = \"What does attention is all you need talk about?\"\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "941b605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to understand what the \"Attention is All You Need\" paper is about. I've heard it's a big deal in AI and machine learning, especially in NLP. Let me start by breaking down the title: \"Attention is All You Need.\" That suggests that attention mechanisms are the core focus here, replacing other components that were previously thought necessary.\n",
      "\n",
      "I remember that before this paper, sequence-to-sequence models with encoder-decoder structures were common. These models used recurrent neural networks (RNNs) or long short-term memory (LSTM) networks. RNNs are good at handling sequential data because they have a memory that captures information from previous inputs. However, I also recall that RNNs have limitations, like vanishing gradients, which make it hard for them to learn long-term dependencies. LSTMs help with that by using memory cells and gates, but they're still not perfect.\n",
      "\n",
      "The paper introduces the Transformer model, which I think relies entirely on attention mechanisms instead of RNNs or LSTMs. So, the key innovation here is the use of self-attention. I'm not entirely sure how self-attention works, but I think it allows the model to attend to all positions in the input sequence simultaneously and weigh their importance. This is different from RNNs, which process sequences step by step, making it harder to capture long-range dependencies efficiently.\n",
      "\n",
      "The Transformer model has an encoder and a decoder, each composed of stacked identical layers. The encoder takes in a sequence of tokens (like words or characters) and outputs a sequence of vectors. The decoder then generates the output sequence, one token at a time, based on the encoder's output. Each layer in both the encoder and decoder has two main sub-layers: multi-head self-attention and position-wise fully connected feed-forward networks.\n",
      "\n",
      "I'm a bit confused about multi-head attention. I think it means the model uses multiple attention mechanisms in parallel, each focusing on different aspects of the data. This allows the model to capture a richer set of contextual relationships. The term \"scaled dot-product attention\" comes up a lot. I believe this refers to the specific way attention is computed, involving dot products scaled by the dimensionality to prevent the values from becoming too small.\n",
      "\n",
      "Positional encoding is another concept I'm trying to grasp. Since the Transformer doesn't use RNNs, it doesn't inherently capture the order of the sequence. Positional encoding adds information about the position of each token in the sequence to the model's input embeddings. This way, the model knows not just what words are present, but also their order. I think the paper uses a combination of sine and cosine functions for positional encoding, which might help in capturing relative positions effectively.\n",
      "\n",
      "The paper also mentions the use of residual connections and layer normalization. Residual connections help with training deep networks by allowing gradients to flow more easily through the network, preventing the vanishing gradient problem. Layer normalization stabilizes the learning process by normalizing the activations of each layer, which can make training faster and more stable.\n",
      "\n",
      "I'm curious about the advantages of the Transformer over traditional RNN-based models. Since the Transformer uses attention mechanisms, it can process all parts of the input sequence in parallel, making it more efficient in terms of computation. RNNs, on the other hand, process sequences sequentially, which can be slower for longer sequences. This parallelization makes the Transformer more suitable for modern computing environments that utilize multi-core processors and GPUs.\n",
      "\n",
      "The paper likely presents experiments showing that the Transformer achieves state-of-the-art results on various machine translation tasks. Machine translation is a common benchmark for sequence-to-sequence models, so it makes sense that the authors would test their model on this task. They probably compare the Transformer's performance against other models, demonstrating that it not only achieves better results but also trains faster.\n",
      "\n",
      "I'm also thinking about the broader impact of this paper. The Transformer architecture has become the foundation for many subsequent models in NLP, like BERT, RoBERTa, and others. These models have revolutionized the field, enabling significant advancements in areas such as text generation, summarization, and question answering. The success of the Transformer can be attributed to its ability to handle long-range dependencies effectively and its efficiency in parallel processing.\n",
      "\n",
      "However, I'm not entirely sure about the limitations of the Transformer model. I think one potential drawback is its computational complexity, especially with regard to memory. The self-attention mechanism requires the model to compute attention scores for all pairs of tokens in the input sequence, which can be memory-intensive for very long sequences. This might limit its application in scenarios where computational resources are constrained.\n",
      "\n",
      "Another thing I'm pondering is how the Transformer handles different types of sequences beyond text. While the paper focuses on machine translation, the architecture seems general enough to be applicable to other domains, like image processing or time-series data. I wonder if the same positional encoding and attention mechanisms can be adapted for these types of data, or if modifications are needed.\n",
      "\n",
      "I'm also trying to recall if the paper introduces any new training techniques or optimizations. The mention of residual connections and layer normalization suggests that the authors paid attention to the training dynamics. Maybe they also discuss strategies for scaling the model, such as increasing the number of layers or attention heads, and how that affects performance.\n",
      "\n",
      "In summary, \"Attention is All You Need\" presents the Transformer model, which replaces traditional RNN-based architectures with self-attention mechanisms. It introduces multi-head attention, positional encoding, and other components that enable efficient and effective sequence modeling. The paper's impact is significant, leading to widespread adoption of Transformer-based architectures in NLP and beyond. While it has its limitations, the Transformer has become a cornerstone in the development of modern AI systems.\n",
      "</think>\n",
      "\n",
      "The paper \"Attention is All You Need\" introduces the Transformer model, which revolutionizes sequence-to-sequence tasks, particularly in natural language processing (NLP). Here's a structured summary of the key points and insights:\n",
      "\n",
      "1. **Introduction and Context**:\n",
      "   - The paper challenges traditional sequence-to-sequence models that rely on RNNs or LSTMs, which have limitations in handling long-term dependencies and parallel processing.\n",
      "\n",
      "2. **Transformer Architecture**:\n",
      "   - The Transformer model is introduced, relying entirely on self-attention mechanisms rather than RNNs or LSTMs.\n",
      "   - It consists of an encoder and decoder, each with stacked layers. Each layer includes multi-head self-attention and position-wise feed-forward networks.\n",
      "\n",
      "3. **Self-Attention Mechanism**:\n",
      "   - Self-attention allows the model to weigh the importance of different parts of the input sequence simultaneously, capturing contextual relationships more effectively.\n",
      "   - Multi-head attention enables the model to focus on different aspects of the data in parallel, enriching contextual understanding.\n",
      "\n",
      "4. **Positional Encoding**:\n",
      "   - Since Transformers don't process sequences sequentially, positional encoding adds information about token positions using sine and cosine functions, helping the model understand sequence order.\n",
      "\n",
      "5. **Training Enhancements**:\n",
      "   - Residual connections and layer normalization are used to facilitate smoother gradient flow and stabilize training, respectively.\n",
      "\n",
      "6. **Advantages**:\n",
      "   - The Transformer's parallel processing capability makes it more efficient and scalable compared to RNN-based models, especially beneficial for modern computing environments.\n",
      "\n",
      "7. **Impact and Applications**:\n",
      "   - The model achieves state-of-the-art results in machine translation and has become foundational for subsequent models like BERT and RoBERTa, impacting various NLP tasks.\n",
      "\n",
      "8. **Limitations**:\n",
      "   - High computational complexity, particularly in memory usage for long sequences, which can be a constraint in resource-limited settings.\n",
      "\n",
      "9. **Broader Implications**:\n",
      "   - The architecture's potential extends beyond text to other domains, though adaptations may be necessary for different data types.\n",
      "\n",
      "In conclusion, \"Attention is All You Need\" presents a transformative approach in NLP, offering efficient and effective sequence modeling through self-attention, significantly influencing AI research and applications.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": question}).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20778df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ConversationalAI3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
