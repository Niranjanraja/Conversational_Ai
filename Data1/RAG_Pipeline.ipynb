{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271d6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1045e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Environment Vars\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"ConversationalAIBot\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9322e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatGroq(\n",
    "    temperature = 0.2,\n",
    "    model_name = \"deepseek-r1-distill-llama-70b\",\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a Research assistant from arxiv\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question = \"What does attention is all you need talk about?\"\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "941b605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to understand what the \"Attention Is All You Need\" paper is about. I remember hearing that it's a big deal in machine learning, especially with the Transformer model. But I'm not exactly sure what it says beyond that. Let me try to break it down.\n",
      "\n",
      "First, the title suggests that attention mechanisms are the key focus here. I know that attention is a concept where the model focuses on different parts of the input when processing each part of the output. But how does that work exactly?\n",
      "\n",
      "The paper is from 2017 by Vaswani et al., right? So it's introducing the Transformer model. I think before Transformers, people used RNNs or LSTMs for sequence tasks like translation. But RNNs have issues with long-term dependencies because they process data sequentially. That's where the Transformer comes in, I guess.\n",
      "\n",
      "The Transformer uses self-attention, which allows the model to look at all parts of the input when generating each part of the output. That should help with capturing long-range dependencies better than RNNs. But how does self-attention work? I remember something about queries, keys, and values. Each word in the input is represented by these vectors, and the attention mechanism computes how much each word should pay attention to others.\n",
      "\n",
      "The paper probably explains how multi-head attention works. That's where the model uses multiple attention mechanisms in parallel, each focusing on different aspects of the data. This helps capture various types of relationships in the input. Then, the outputs from each head are combined to get a more comprehensive representation.\n",
      "\n",
      "I also recall that the Transformer doesn't use convolution or recurrence, which means it can process all parts of the input in parallel. This should make training faster compared to RNNs, which are sequential. But without recurrence, how does the model handle the order of the sequence? Oh, right, they use positional encodings to add information about the position of each word in the sequence. That makes sense because without it, the model wouldn't know the order of the words.\n",
      "\n",
      "The architecture itself is encoder-decoder based. The encoder takes in a sequence and generates an encoding, and the decoder generates the output based on that encoding. Each encoder layer has self-attention and a feed-forward network. The decoder layers do the same but also attend to the encoder's output. Plus, there's this multi-head attention in the decoder that looks at the encoder's output to generate each token.\n",
      "\n",
      "I think the paper also introduces scaled dot-product attention. The idea is to compute attention scores by taking the dot product of the query and key vectors, then scaling them. This scaling helps prevent the scores from being too large, which could make the softmax function less effective. They also apply softmax to these scores to get the final attention weights.\n",
      "\n",
      "The Transformer was tested on machine translation tasks, like English to German translation. They used the WMT dataset and achieved state-of-the-art results at the time. That's impressive because it showed that the Transformer could outperform other models without using any recurrence or convolution.\n",
      "\n",
      "I'm a bit fuzzy on the details of the positional encoding. I think they use sinusoidal functions to encode the position information. Each position is embedded with a vector that encodes its position, allowing the model to understand the order of the sequence. This is crucial because the self-attention mechanism alone doesn't capture the position information.\n",
      "\n",
      "Another thing I'm not entirely clear on is how the multi-head attention combines the different attention heads. I believe each head has its own set of weights, so they can learn different aspects of the data. After computing the attention for each head, the outputs are concatenated and linearly transformed to form the final output of the multi-head attention.\n",
      "\n",
      "The paper also mentions the use of residual connections and layer normalization. These help with training deep networks by preventing the gradients from vanishing or exploding. Each sub-layer (like multi-head attention and the feed-forward network) is followed by a residual connection and layer normalization to stabilize the training process.\n",
      "\n",
      "I'm curious about the training process. They probably used a large-scale dataset and distributed training to handle the computational demands. The Transformer's architecture allows for parallelization, which makes training more efficient compared to RNNs.\n",
      "\n",
      "In summary, the \"Attention Is All You Need\" paper introduces the Transformer model, which relies entirely on self-attention mechanisms. It explains how the model processes input sequences in parallel using multi-head attention, incorporates positional encodings, and uses an encoder-decoder structure. The paper demonstrates the effectiveness of the Transformer on machine translation tasks, showing that attention mechanisms can achieve state-of-the-art results without the need for recurrence or convolution.\n",
      "\n",
      "I think I've got a basic understanding, but I should probably read the paper or a detailed summary to fill in the gaps, especially regarding the mathematical details of the attention mechanism and the specific architectures used in the encoder and decoder layers.\n",
      "</think>\n",
      "\n",
      "The \"Attention Is All You Need\" paper, authored by Vaswani et al. in 2017, introduces the Transformer model, which revolutionizes sequence-to-sequence tasks like machine translation by relying solely on attention mechanisms. Here's a structured summary of the key concepts and contributions:\n",
      "\n",
      "1. **Introduction of the Transformer Model**: The paper presents the Transformer, which replaces traditional RNNs and LSTMs with self-attention mechanisms, enabling parallel processing and better handling of long-range dependencies.\n",
      "\n",
      "2. **Self-Attention Mechanism**: The core of the Transformer is self-attention, where each word in the input attends to all others. This is computed using query, key, and value vectors, allowing the model to weigh the importance of different words dynamically.\n",
      "\n",
      "3. **Multi-Head Attention**: The model employs multiple attention heads in parallel, each capturing different aspects of the data. Outputs from these heads are concatenated and linearly transformed to form a comprehensive representation.\n",
      "\n",
      "4. **Positional Encoding**: Since the Transformer doesn't use recurrence, positional encodings (using sinusoidal functions) are added to the input embeddings to preserve sequence order information.\n",
      "\n",
      "5. **Encoder-Decoder Architecture**: The model consists of an encoder and decoder. Each encoder layer includes self-attention and a feed-forward network. The decoder layers additionally use attention to the encoder's output.\n",
      "\n",
      "6. **Scaled Dot-Product Attention**: This mechanism scales the dot product of queries and keys to prevent large values, applying softmax to obtain attention weights.\n",
      "\n",
      "7. **State-of-the-Art Results**: The Transformer outperformed existing models on the WMT English-German translation task, demonstrating its effectiveness without recurrence or convolution.\n",
      "\n",
      "8. **Training Efficiency**: The architecture allows parallel processing, making training more efficient. Techniques like residual connections and layer normalization stabilize deep network training.\n",
      "\n",
      "In essence, the paper highlights how attention mechanisms can achieve superior results in sequence tasks, setting a new standard for architectures in natural language processing.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": question}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749a010",
   "metadata": {},
   "source": [
    "#### Loading the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a20778df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>intent</th>\n",
       "      <th>category</th>\n",
       "      <th>tags</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there are charges on my phone bill that i do n...</td>\n",
       "      <td>dispute_invoice</td>\n",
       "      <td>BILLING</td>\n",
       "      <td>BCELQZ</td>\n",
       "      <td>If you have noticed discrepancies in your bill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my internet bill is incorrect, can you help me...</td>\n",
       "      <td>dispute_invoice</td>\n",
       "      <td>BILLING</td>\n",
       "      <td>BCILZ</td>\n",
       "      <td>If there is an issue with your bill and you wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my invoice is not corect challenge it</td>\n",
       "      <td>dispute_invoice</td>\n",
       "      <td>BILLING</td>\n",
       "      <td>BCELQZ</td>\n",
       "      <td>If you find any discrepancies in your invoice ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don't recognize some charges on my bill, can...</td>\n",
       "      <td>dispute_invoice</td>\n",
       "      <td>BILLING</td>\n",
       "      <td>BCIL</td>\n",
       "      <td>If you have found charges on your bill that yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i have to dispute a fucking bill i need help</td>\n",
       "      <td>dispute_invoice</td>\n",
       "      <td>BILLING</td>\n",
       "      <td>BCQW</td>\n",
       "      <td>If you believe there is an error on your bill ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction           intent  \\\n",
       "0  there are charges on my phone bill that i do n...  dispute_invoice   \n",
       "1  my internet bill is incorrect, can you help me...  dispute_invoice   \n",
       "2              my invoice is not corect challenge it  dispute_invoice   \n",
       "3  I don't recognize some charges on my bill, can...  dispute_invoice   \n",
       "4       i have to dispute a fucking bill i need help  dispute_invoice   \n",
       "\n",
       "  category    tags                                           response  \n",
       "0  BILLING  BCELQZ  If you have noticed discrepancies in your bill...  \n",
       "1  BILLING   BCILZ  If there is an issue with your bill and you wo...  \n",
       "2  BILLING  BCELQZ  If you find any discrepancies in your invoice ...  \n",
       "3  BILLING    BCIL  If you have found charges on your bill that yo...  \n",
       "4  BILLING    BCQW  If you believe there is an error on your bill ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "df = pd.read_csv(\"bitext-telco-llm-chatbot-training-dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Itering rows for organizing: Tried with classical CSV loader, this was efficient\n",
    "conversation_documents = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    content = f\"\"\"\"Instruction: \n",
    "    {row['instruction']}\n",
    "    Intent: {row['intent']}\n",
    "    Category: {row['category']}\n",
    "    Tags: {row['tags']}\n",
    "    Response: {row['response']}\"\"\"\n",
    "\n",
    "    conversation_documents.append(Document(page_content=content, metadata={\"intent\": row['intent']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "000eeee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\"Instruction: \n",
      "    there are charges on my phone bill that i do not recogniae challenge it\n",
      "    Intent: dispute_invoice\n",
      "    Category: BILLING\n",
      "    Tags: BCELQZ\n",
      "    Response: If you have noticed discrepancies in your bill and wish to contest them, please adhere to the following steps:\n",
      "\n",
      "1. Log in to your account on {{WEBSITE_URL}}.\n",
      "2. Navigate to the {{INVOICE_SECTION}} section.\n",
      "3. Select the bill you wish to dispute.\n",
      "4. Click on the {{DISPUTE_INVOICE_OPTION}} to dispute the charge.\n",
      "5. Fill in the required information and submit your dispute.\n",
      "\n",
      "Our team will review your submission and get back to you within {{DAYS_NUMBER}} business days.' metadata={'intent': 'dispute_invoice'}\n"
     ]
    }
   ],
   "source": [
    "print(conversation_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing and Storing Faiss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ConversationalAI3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
